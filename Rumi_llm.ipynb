{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5u65-vLqktp",
        "outputId": "db2e6795-7d5b-4450-da34-6e772c1d1c6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# =========================================================\n",
        "# CONFIG\n",
        "DATA_PATH = \"/content/drive/MyDrive/rumi_small_clean.txt\"\n",
        "\n",
        "# Choose tokenizer: \"char\" (simplest baseline) or \"bpe\" (educational)\n",
        "TOKENIZER_TYPE = \"char\"\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 256\n",
        "max_iters = 30000\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "\n",
        "n_embd = 128\n",
        "n_head = 4\n",
        "n_layer = 6\n",
        "\n",
        "dropout = 0.1\n",
        "weight_decay = 0.1\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 1000\n",
        "\n",
        "# BPE settings (only if TOKENIZER_TYPE=\"bpe\")\n",
        "BPE_VOCAB_SIZE = 2048\n",
        "BPE_MIN_PAIR_FREQ = 2\n",
        "\n",
        "# Early stopping / checkpoint\n",
        "patience_evals = 20\n",
        "min_delta = 1e-3\n",
        "min_iters_before_stop = 3000\n",
        "\n",
        "# Resume option (set True to continue from last.pt)\n",
        "RESUME_FROM_LAST = False\n",
        "\n",
        "# Checkpoints dir\n",
        "ckpt_dir = \"/content/drive/MyDrive/rumi_checkpoints\"\n",
        "# =========================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(1337)\n",
        "\n",
        "# device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# LR schedule: warmup + cosine decay\n",
        "def get_lr(it: int) -> float:\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / max(1, warmup_iters)\n",
        "    progress = (it - warmup_iters) / max(1, (max_iters - warmup_iters))\n",
        "    return learning_rate * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "# ----------------------------\n",
        "# Better Persian preprocessing\n",
        "def clean_persian_text(t: str) -> str:\n",
        "    # 1) unify Arabic/Persian letters\n",
        "    t = t.replace('Ÿä', '€å').replace('ŸÉ', '⁄©')\n",
        "    t = t.replace('ÿ©', 'Ÿá').replace('ÿ§', 'Ÿà').replace('ÿ£', 'ÿß').replace('ÿ•', 'ÿß').replace('Ÿ±', 'ÿß')\n",
        "\n",
        "    # 2) remove diacritics (harakat) + tatweel\n",
        "    t = re.sub(r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]', '', t)  # diacritics ranges\n",
        "    t = t.replace('\\u0640', '')  # tatweel ŸÄ\n",
        "\n",
        "    # 3) normalize ZWNJ (half-space)\n",
        "    t = t.replace('\\u200c', '‚Äå')\n",
        "\n",
        "    # 4) remove direction marks (invisible RTL/LTR marks)\n",
        "    t = re.sub(r'[\\u200e\\u200f\\u202a-\\u202e\\u2066-\\u2069]', '', t)\n",
        "\n",
        "    # 5) normalize line endings\n",
        "    t = t.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "\n",
        "    # 6) normalize spaces\n",
        "    t = re.sub(r'[ \\t]+', ' ', t)\n",
        "    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
        "\n",
        "    # 7) light punctuation spacing (optional but helpful)\n",
        "    # remove space before punctuation\n",
        "    t = re.sub(r'\\s+([ÿåÿõ:\\.\\!\\ÿü])', r'\\1', t)\n",
        "    # ensure a space after punctuation if next is not whitespace/newline\n",
        "    t = re.sub(r'([ÿåÿõ:\\.\\!\\ÿü])([^\\s\\n])', r'\\1 \\2', t)\n",
        "\n",
        "    return t\n",
        "\n",
        "# ----------------------------\n",
        "# Load data\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"DATA_PATH not found: {DATA_PATH}\")\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = clean_persian_text(f.read())\n",
        "\n",
        "# =========================================================\n",
        "# TOKENIZERS\n",
        "# =========================================================\n",
        "\n",
        "# -------- Char tokenizer (simplest baseline)\n",
        "class CharTokenizer:\n",
        "    def __init__(self, vocab, unk_token=\"ÔøΩ\"):\n",
        "        self.unk_token = unk_token\n",
        "        if unk_token not in vocab:\n",
        "            vocab = [unk_token] + vocab\n",
        "        self.vocab = vocab\n",
        "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "        self.unk_id = self.stoi[unk_token]\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def encode(self, s: str):\n",
        "        return [self.stoi.get(ch, self.unk_id) for ch in s]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join(self.itos.get(i, self.unk_token) for i in ids)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"vocab\": self.vocab, \"unk_token\": self.unk_token}, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            obj = json.load(f)\n",
        "        return cls(obj[\"vocab\"], unk_token=obj.get(\"unk_token\", \"ÔøΩ\"))\n",
        "\n",
        "# -------- Simple BPE (educational)\n",
        "def _merge_seq(seq, pair, new_id):\n",
        "    a, b = pair\n",
        "    out = []\n",
        "    i = 0\n",
        "    L = len(seq)\n",
        "    while i < L:\n",
        "        if i < L - 1 and seq[i] == a and seq[i + 1] == b:\n",
        "            out.append(new_id)\n",
        "            i += 2\n",
        "        else:\n",
        "            out.append(seq[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "class SimpleBPETokenizer:\n",
        "    def __init__(self, id_to_token, merges, unk_token='ÔøΩ'):\n",
        "        self.id_to_token = dict(id_to_token)\n",
        "        self.token_to_id = {t: i for i, t in self.id_to_token.items()}\n",
        "        self.merges = list(merges)  # (a_id, b_id, new_id)\n",
        "        self.unk_token = unk_token\n",
        "        self.unk_id = self.token_to_id.get(unk_token, None)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.id_to_token)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pretokenize(t):\n",
        "        return re.findall(r\"\\s+|\\S+\", t)\n",
        "\n",
        "    @classmethod\n",
        "    def train(cls, text, vocab_size=2048, min_pair_freq=2, return_encoded=True, unk_token='ÔøΩ'):\n",
        "        parts = cls._pretokenize(text)\n",
        "\n",
        "        chars = sorted(set(text))\n",
        "        if unk_token not in chars:\n",
        "            chars = [unk_token] + chars\n",
        "\n",
        "        token_to_id = {ch: i for i, ch in enumerate(chars)}\n",
        "        id_to_token = {i: ch for ch, i in token_to_id.items()}\n",
        "\n",
        "        seqs = [[token_to_id.get(ch, token_to_id[unk_token]) for ch in p] for p in parts]\n",
        "\n",
        "        merges = []\n",
        "        next_id = len(id_to_token)\n",
        "\n",
        "        while next_id < vocab_size:\n",
        "            pair_counts = {}\n",
        "            for seq in seqs:\n",
        "                for a, b in zip(seq, seq[1:]):\n",
        "                    pair_counts[(a, b)] = pair_counts.get((a, b), 0) + 1\n",
        "\n",
        "            if not pair_counts:\n",
        "                break\n",
        "\n",
        "            (best_a, best_b), best_cnt = max(pair_counts.items(), key=lambda kv: (kv[1], kv[0]))\n",
        "            if best_cnt < min_pair_freq:\n",
        "                break\n",
        "\n",
        "            new_id = next_id\n",
        "            next_id += 1\n",
        "\n",
        "            id_to_token[new_id] = id_to_token[best_a] + id_to_token[best_b]\n",
        "            merges.append((best_a, best_b, new_id))\n",
        "\n",
        "            pair = (best_a, best_b)\n",
        "            seqs = [_merge_seq(seq, pair, new_id) for seq in seqs]\n",
        "\n",
        "        tok = cls(id_to_token=id_to_token, merges=merges, unk_token=unk_token)\n",
        "\n",
        "        if return_encoded:\n",
        "            encoded = [tid for seq in seqs for tid in seq]\n",
        "            return tok, encoded\n",
        "        return tok\n",
        "\n",
        "    def encode(self, s):\n",
        "        parts = self._pretokenize(s)\n",
        "        ids = []\n",
        "        for p in parts:\n",
        "            seq = [self.token_to_id.get(ch, self.unk_id) for ch in p]\n",
        "            for a, b, new_id in self.merges:\n",
        "                seq = _merge_seq(seq, (a, b), new_id)\n",
        "            ids.extend(seq)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join(self.id_to_token.get(i, self.unk_token) for i in ids)\n",
        "\n",
        "    def save(self, path):\n",
        "        obj = {\n",
        "            \"id_to_token\": {str(i): t for i, t in self.id_to_token.items()},\n",
        "            \"merges\": self.merges,\n",
        "            \"unk_token\": self.unk_token,\n",
        "        }\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(obj, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            obj = json.load(f)\n",
        "        id_to_token = {int(i): t for i, t in obj[\"id_to_token\"].items()}\n",
        "        merges = [tuple(m) for m in obj[\"merges\"]]\n",
        "        return cls(id_to_token=id_to_token, merges=merges, unk_token=obj.get(\"unk_token\", \"ÔøΩ\"))\n",
        "\n",
        "# =========================================================\n",
        "# Build/load tokenizer + cache encoded tokens\n",
        "# =========================================================\n",
        "\n",
        "CACHE_TAG = \"v2_clean\"  # bump this if you change preprocessing/tokenizer logic\n",
        "\n",
        "if TOKENIZER_TYPE == \"char\":\n",
        "    TOK_PATH = f\"/content/drive/MyDrive/rumi_char_tokenizer_{CACHE_TAG}.json\"\n",
        "    ENC_PATH = f\"/content/drive/MyDrive/rumi_encoded_char_{CACHE_TAG}.pt\"\n",
        "\n",
        "    if os.path.exists(TOK_PATH):\n",
        "        tokenizer = CharTokenizer.load(TOK_PATH)\n",
        "        print(\"Loaded char tokenizer:\", TOK_PATH)\n",
        "    else:\n",
        "        vocab = sorted(set(text))\n",
        "        tokenizer = CharTokenizer(vocab=vocab, unk_token=\"ÔøΩ\")\n",
        "        tokenizer.save(TOK_PATH)\n",
        "        print(\"Built + saved char tokenizer:\", TOK_PATH)\n",
        "\n",
        "    if os.path.exists(ENC_PATH):\n",
        "        data = torch.load(ENC_PATH, map_location=\"cpu\")\n",
        "        print(\"Loaded encoded tokens:\", ENC_PATH)\n",
        "    else:\n",
        "        data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "        torch.save(data, ENC_PATH)\n",
        "        print(\"Encoded + saved tokens:\", ENC_PATH)\n",
        "\n",
        "elif TOKENIZER_TYPE == \"bpe\":\n",
        "    TOK_PATH = f\"/content/drive/MyDrive/rumi_bpe_tokenizer_bpe{BPE_VOCAB_SIZE}_min{BPE_MIN_PAIR_FREQ}_{CACHE_TAG}.json\"\n",
        "    ENC_PATH = f\"/content/drive/MyDrive/rumi_encoded_bpe{BPE_VOCAB_SIZE}_min{BPE_MIN_PAIR_FREQ}_{CACHE_TAG}.pt\"\n",
        "\n",
        "    if os.path.exists(TOK_PATH):\n",
        "        tokenizer = SimpleBPETokenizer.load(TOK_PATH)\n",
        "        print(\"Loaded BPE tokenizer:\", TOK_PATH)\n",
        "    else:\n",
        "        tokenizer, _ = SimpleBPETokenizer.train(\n",
        "            text,\n",
        "            vocab_size=BPE_VOCAB_SIZE,\n",
        "            min_pair_freq=BPE_MIN_PAIR_FREQ,\n",
        "            return_encoded=True,\n",
        "        )\n",
        "        tokenizer.save(TOK_PATH)\n",
        "        print(\"Trained + saved BPE tokenizer:\", TOK_PATH)\n",
        "\n",
        "    if os.path.exists(ENC_PATH):\n",
        "        data = torch.load(ENC_PATH, map_location=\"cpu\")\n",
        "        print(\"Loaded encoded tokens:\", ENC_PATH)\n",
        "    else:\n",
        "        data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "        torch.save(data, ENC_PATH)\n",
        "        print(\"Encoded + saved tokens:\", ENC_PATH)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"TOKENIZER_TYPE must be 'char' or 'bpe'\")\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "encode = tokenizer.encode\n",
        "decode = tokenizer.decode\n",
        "\n",
        "print(\"tokens:\", len(data), \"vocab_size:\", vocab_size)\n",
        "if len(data) <= block_size:\n",
        "    raise ValueError(f\"len(data)={len(data)} must be > block_size={block_size}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Train/val split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Sampling: top-p (nucleus)\n",
        "def top_p_filtering(logits, top_p=0.9):\n",
        "    sorted_logits, sorted_idx = torch.sort(logits, descending=True, dim=-1)\n",
        "    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "    cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    mask = cumprobs > top_p\n",
        "    mask[..., 0] = False\n",
        "    sorted_logits = sorted_logits.masked_fill(mask, float('-inf'))\n",
        "    filtered = torch.zeros_like(logits).scatter(-1, sorted_idx, sorted_logits)\n",
        "    return filtered\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # bias=False is cleaner for weight tying\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embedding_table.weight  # weight tying\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(B * T, -1), targets.view(B * T))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.8, top_p=0.9):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            logits = logits / max(temperature, 1e-6)\n",
        "            logits = top_p_filtering(logits, top_p=top_p)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    betas=(0.9, 0.95)\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters, device='cpu')\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)   # logits ÿ±ÿß ŸÜ⁄ØŸá ŸÖ€å‚ÄåÿØÿßÿ±€åŸÖ ŸàŸÑ€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÜŸÖ€å‚Äå⁄©ŸÜ€åŸÖ\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Checkpointing + Early Stopping\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "best_ckpt_path = os.path.join(ckpt_dir, f\"best_{TOKENIZER_TYPE}_{CACHE_TAG}.pt\")\n",
        "last_ckpt_path = os.path.join(ckpt_dir, f\"last_{TOKENIZER_TYPE}_{CACHE_TAG}.pt\")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "bad_evals = 0\n",
        "start_iter = 0\n",
        "\n",
        "def save_checkpoint(path, iter_idx, best_val_loss, is_best=False):\n",
        "    payload = {\n",
        "        \"iter\": iter_idx,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if (device == \"cuda\") else None,\n",
        "        \"best_val\": best_val_loss,\n",
        "        \"meta\": {\n",
        "            \"tokenizer_type\": TOKENIZER_TYPE,\n",
        "            \"cache_tag\": CACHE_TAG,\n",
        "        },\n",
        "        \"config\": {\n",
        "            \"batch_size\": batch_size,\n",
        "            \"block_size\": block_size,\n",
        "            \"max_iters\": max_iters,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"n_embd\": n_embd,\n",
        "            \"n_head\": n_head,\n",
        "            \"n_layer\": n_layer,\n",
        "            \"dropout\": dropout,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"grad_clip\": grad_clip,\n",
        "            \"warmup_iters\": warmup_iters,\n",
        "            \"BPE_VOCAB_SIZE\": BPE_VOCAB_SIZE,\n",
        "            \"BPE_MIN_PAIR_FREQ\": BPE_MIN_PAIR_FREQ,\n",
        "        }\n",
        "    }\n",
        "    torch.save(payload, path)\n",
        "    print((\"‚úÖ Saved BEST -> \" if is_best else \"üíæ Saved -> \") + path)\n",
        "\n",
        "def try_resume(path):\n",
        "    global best_val, start_iter, bad_evals\n",
        "    if os.path.exists(path):\n",
        "        ckpt = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        if device == \"cuda\" and ckpt.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "        best_val = float(ckpt.get(\"best_val\", float(\"inf\")))\n",
        "        start_iter = int(ckpt.get(\"iter\", 0)) + 1\n",
        "        bad_evals = 0\n",
        "        print(f\"üîÅ Resumed from {path} | start_iter={start_iter} | best_val={best_val:.4f}\")\n",
        "\n",
        "if RESUME_FROM_LAST:\n",
        "    try_resume(last_ckpt_path)\n",
        "\n",
        "# ----------------------------\n",
        "# TRAIN LOOP\n",
        "for it in range(start_iter, max_iters):\n",
        "    lr = get_lr(it)\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = lr\n",
        "\n",
        "    if it % eval_interval == 0 or it == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = float(losses[\"train\"])\n",
        "        val_loss = float(losses[\"val\"])\n",
        "        print(f\"step {it}: lr {lr:.2e} | train {train_loss:.4f} | val {val_loss:.4f}\")\n",
        "\n",
        "        save_checkpoint(last_ckpt_path, it, best_val, is_best=False)\n",
        "\n",
        "        if val_loss < best_val - min_delta:\n",
        "            best_val = val_loss\n",
        "            bad_evals = 0\n",
        "            save_checkpoint(best_ckpt_path, it, best_val, is_best=True)\n",
        "        else:\n",
        "            bad_evals += 1\n",
        "\n",
        "        if it >= min_iters_before_stop and bad_evals >= patience_evals:\n",
        "            print(f\"‚õî Early stopping: no val improvement for {patience_evals} evals.\")\n",
        "            break\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "        _, loss = model(xb, yb)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "print(\"‚úÖ Training finished. Best val loss:\", best_val)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy0FfViBqe2Y",
        "outputId": "e9b82f76-9c6a-4043-d729-1b9867aa2629"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "GPU: Tesla T4\n",
            "Loaded char tokenizer: /content/drive/MyDrive/rumi_char_tokenizer_v2_clean.json\n",
            "Loaded encoded tokens: /content/drive/MyDrive/rumi_encoded_char_v2_clean.pt\n",
            "tokens: 783029 vocab_size: 55\n",
            "1.227392 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1081275461.py:457: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: lr 0.00e+00 | train 85.3806 | val 85.1495\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1081275461.py:558: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: lr 6.00e-05 | train 8.3882 | val 8.5606\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 400: lr 1.20e-04 | train 2.7198 | val 2.8002\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 600: lr 1.80e-04 | train 2.6269 | val 2.7039\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 800: lr 2.40e-04 | train 2.6115 | val 2.6880\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 1000: lr 3.00e-04 | train 2.5813 | val 2.6576\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 1200: lr 3.00e-04 | train 2.5750 | val 2.6552\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 1400: lr 3.00e-04 | train 2.5673 | val 2.6468\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 1600: lr 3.00e-04 | train 2.5590 | val 2.6335\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 1800: lr 2.99e-04 | train 2.5337 | val 2.6105\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 2000: lr 2.99e-04 | train 2.5210 | val 2.6003\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 2200: lr 2.99e-04 | train 2.5049 | val 2.5844\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 2400: lr 2.98e-04 | train 2.4836 | val 2.5619\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 2600: lr 2.98e-04 | train 2.4606 | val 2.5434\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 2800: lr 2.97e-04 | train 2.4267 | val 2.5097\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 3000: lr 2.96e-04 | train 2.3904 | val 2.4756\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 3200: lr 2.96e-04 | train 2.3630 | val 2.4466\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 3400: lr 2.95e-04 | train 2.3323 | val 2.4186\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 3600: lr 2.94e-04 | train 2.2987 | val 2.3830\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 3800: lr 2.93e-04 | train 2.2605 | val 2.3496\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 4000: lr 2.92e-04 | train 2.2287 | val 2.3187\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 4200: lr 2.91e-04 | train 2.2103 | val 2.2998\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 4400: lr 2.90e-04 | train 2.1802 | val 2.2753\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 4600: lr 2.89e-04 | train 2.1503 | val 2.2439\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 4800: lr 2.87e-04 | train 2.1234 | val 2.2143\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 5000: lr 2.86e-04 | train 2.0961 | val 2.1941\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 5200: lr 2.85e-04 | train 2.0715 | val 2.1712\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 5400: lr 2.83e-04 | train 2.0527 | val 2.1439\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 5600: lr 2.82e-04 | train 2.0269 | val 2.1239\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 5800: lr 2.80e-04 | train 2.0107 | val 2.1028\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 6000: lr 2.79e-04 | train 1.9855 | val 2.0894\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 6200: lr 2.77e-04 | train 1.9688 | val 2.0638\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 6400: lr 2.75e-04 | train 1.9491 | val 2.0436\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 6600: lr 2.73e-04 | train 1.9321 | val 2.0352\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 6800: lr 2.71e-04 | train 1.9170 | val 2.0235\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 7000: lr 2.69e-04 | train 1.9044 | val 2.0061\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 7200: lr 2.67e-04 | train 1.8869 | val 1.9937\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 7400: lr 2.65e-04 | train 1.8724 | val 1.9859\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 7600: lr 2.63e-04 | train 1.8627 | val 1.9707\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 7800: lr 2.61e-04 | train 1.8507 | val 1.9585\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 8000: lr 2.59e-04 | train 1.8447 | val 1.9565\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 8200: lr 2.57e-04 | train 1.8267 | val 1.9461\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 8400: lr 2.54e-04 | train 1.8199 | val 1.9389\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 8600: lr 2.52e-04 | train 1.8084 | val 1.9271\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 8800: lr 2.50e-04 | train 1.7976 | val 1.9238\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 9000: lr 2.47e-04 | train 1.7935 | val 1.9148\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 9200: lr 2.45e-04 | train 1.7835 | val 1.9120\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 9400: lr 2.42e-04 | train 1.7791 | val 1.9071\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 9600: lr 2.39e-04 | train 1.7699 | val 1.8971\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 9800: lr 2.37e-04 | train 1.7642 | val 1.8884\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 10000: lr 2.34e-04 | train 1.7567 | val 1.8864\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 10200: lr 2.31e-04 | train 1.7521 | val 1.8834\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 10400: lr 2.29e-04 | train 1.7487 | val 1.8791\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 10600: lr 2.26e-04 | train 1.7378 | val 1.8780\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 10800: lr 2.23e-04 | train 1.7336 | val 1.8744\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 11000: lr 2.20e-04 | train 1.7292 | val 1.8680\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 11200: lr 2.17e-04 | train 1.7231 | val 1.8642\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 11400: lr 2.14e-04 | train 1.7148 | val 1.8656\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 11600: lr 2.12e-04 | train 1.7098 | val 1.8595\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 11800: lr 2.09e-04 | train 1.7075 | val 1.8501\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 12000: lr 2.06e-04 | train 1.7003 | val 1.8522\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 12200: lr 2.02e-04 | train 1.6962 | val 1.8473\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 12400: lr 1.99e-04 | train 1.6919 | val 1.8485\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 12600: lr 1.96e-04 | train 1.6866 | val 1.8405\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 12800: lr 1.93e-04 | train 1.6835 | val 1.8393\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 13000: lr 1.90e-04 | train 1.6750 | val 1.8341\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 13200: lr 1.87e-04 | train 1.6743 | val 1.8434\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 13400: lr 1.84e-04 | train 1.6703 | val 1.8342\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 13600: lr 1.81e-04 | train 1.6667 | val 1.8355\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 13800: lr 1.77e-04 | train 1.6657 | val 1.8303\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 14000: lr 1.74e-04 | train 1.6599 | val 1.8249\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 14200: lr 1.71e-04 | train 1.6573 | val 1.8221\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 14400: lr 1.68e-04 | train 1.6488 | val 1.8263\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 14600: lr 1.65e-04 | train 1.6479 | val 1.8230\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 14800: lr 1.61e-04 | train 1.6441 | val 1.8201\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 15000: lr 1.58e-04 | train 1.6426 | val 1.8195\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 15200: lr 1.55e-04 | train 1.6347 | val 1.8146\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 15400: lr 1.52e-04 | train 1.6365 | val 1.8177\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 15600: lr 1.48e-04 | train 1.6312 | val 1.8126\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 15800: lr 1.45e-04 | train 1.6322 | val 1.8150\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 16000: lr 1.42e-04 | train 1.6251 | val 1.8144\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 16200: lr 1.39e-04 | train 1.6212 | val 1.8135\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 16400: lr 1.35e-04 | train 1.6174 | val 1.8123\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 16600: lr 1.32e-04 | train 1.6146 | val 1.8078\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 16800: lr 1.29e-04 | train 1.6155 | val 1.8078\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 17000: lr 1.26e-04 | train 1.6149 | val 1.8060\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 17200: lr 1.23e-04 | train 1.6112 | val 1.8058\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 17400: lr 1.19e-04 | train 1.6064 | val 1.8038\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 17600: lr 1.16e-04 | train 1.6102 | val 1.8050\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 17800: lr 1.13e-04 | train 1.6045 | val 1.8010\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 18000: lr 1.10e-04 | train 1.6008 | val 1.8002\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 18200: lr 1.07e-04 | train 1.5967 | val 1.8040\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 18400: lr 1.04e-04 | train 1.5993 | val 1.7975\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 18600: lr 1.01e-04 | train 1.5939 | val 1.7990\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 18800: lr 9.75e-05 | train 1.5969 | val 1.7990\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 19000: lr 9.45e-05 | train 1.5847 | val 1.7994\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 19200: lr 9.15e-05 | train 1.5908 | val 1.8009\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 19400: lr 8.85e-05 | train 1.5893 | val 1.8035\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 19600: lr 8.55e-05 | train 1.5887 | val 1.7974\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 19800: lr 8.26e-05 | train 1.5862 | val 1.8001\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 20000: lr 7.97e-05 | train 1.5851 | val 1.7915\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 20200: lr 7.69e-05 | train 1.5824 | val 1.7933\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 20400: lr 7.41e-05 | train 1.5837 | val 1.7927\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 20600: lr 7.13e-05 | train 1.5798 | val 1.7934\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 20800: lr 6.85e-05 | train 1.5785 | val 1.7961\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 21000: lr 6.58e-05 | train 1.5782 | val 1.7932\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 21200: lr 6.32e-05 | train 1.5776 | val 1.7893\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 21400: lr 6.05e-05 | train 1.5717 | val 1.7864\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 21600: lr 5.79e-05 | train 1.5708 | val 1.7937\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 21800: lr 5.54e-05 | train 1.5716 | val 1.7892\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 22000: lr 5.29e-05 | train 1.5711 | val 1.7930\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 22200: lr 5.04e-05 | train 1.5666 | val 1.7927\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 22400: lr 4.80e-05 | train 1.5695 | val 1.7901\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 22600: lr 4.57e-05 | train 1.5661 | val 1.7881\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 22800: lr 4.34e-05 | train 1.5639 | val 1.7929\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 23000: lr 4.11e-05 | train 1.5634 | val 1.7877\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 23200: lr 3.89e-05 | train 1.5633 | val 1.7913\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 23400: lr 3.67e-05 | train 1.5635 | val 1.7872\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 23600: lr 3.46e-05 | train 1.5645 | val 1.7851\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 23800: lr 3.26e-05 | train 1.5633 | val 1.7864\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 24000: lr 3.06e-05 | train 1.5631 | val 1.7901\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 24200: lr 2.86e-05 | train 1.5628 | val 1.7912\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 24400: lr 2.68e-05 | train 1.5582 | val 1.7866\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 24600: lr 2.49e-05 | train 1.5598 | val 1.7905\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 24800: lr 2.32e-05 | train 1.5595 | val 1.7878\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 25000: lr 2.15e-05 | train 1.5566 | val 1.7876\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 25200: lr 1.98e-05 | train 1.5552 | val 1.7880\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 25400: lr 1.82e-05 | train 1.5576 | val 1.7872\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 25600: lr 1.67e-05 | train 1.5555 | val 1.7863\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 25800: lr 1.53e-05 | train 1.5568 | val 1.7880\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 26000: lr 1.39e-05 | train 1.5554 | val 1.7840\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 26200: lr 1.25e-05 | train 1.5578 | val 1.7829\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Saved BEST -> /content/drive/MyDrive/rumi_checkpoints/best_char_v2_clean.pt\n",
            "step 26400: lr 1.13e-05 | train 1.5534 | val 1.7897\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 26600: lr 1.01e-05 | train 1.5564 | val 1.7872\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 26800: lr 8.92e-06 | train 1.5554 | val 1.7875\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 27000: lr 7.85e-06 | train 1.5544 | val 1.7845\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 27200: lr 6.85e-06 | train 1.5535 | val 1.7846\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 27400: lr 5.91e-06 | train 1.5549 | val 1.7844\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 27600: lr 5.04e-06 | train 1.5523 | val 1.7859\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 27800: lr 4.24e-06 | train 1.5512 | val 1.7891\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 28000: lr 3.51e-06 | train 1.5533 | val 1.7840\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 28200: lr 2.84e-06 | train 1.5531 | val 1.7857\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 28400: lr 2.25e-06 | train 1.5563 | val 1.7838\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 28600: lr 1.72e-06 | train 1.5533 | val 1.7869\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 28800: lr 1.27e-06 | train 1.5541 | val 1.7856\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 29000: lr 8.79e-07 | train 1.5534 | val 1.7864\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 29200: lr 5.63e-07 | train 1.5554 | val 1.7843\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 29400: lr 3.17e-07 | train 1.5590 | val 1.7853\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 29600: lr 1.41e-07 | train 1.5508 | val 1.7850\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 29800: lr 3.52e-08 | train 1.5516 | val 1.7847\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "step 29999: lr 8.80e-13 | train 1.5553 | val 1.7863\n",
            "üíæ Saved -> /content/drive/MyDrive/rumi_checkpoints/last_char_v2_clean.pt\n",
            "‚úÖ Training finished. Best val loss: 1.7829126119613647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# GENERATE\n",
        "# =========================\n",
        "import os\n",
        "import torch\n",
        "\n",
        "best_ckpt_path = os.path.join(ckpt_dir, f\"best_{TOKENIZER_TYPE}_{CACHE_TAG}.pt\")\n",
        "\n",
        "if not os.path.exists(best_ckpt_path):\n",
        "    raise FileNotFoundError(f\"Best checkpoint not found: {best_ckpt_path}\")\n",
        "\n",
        "ckpt = torch.load(best_ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.eval()\n",
        "print(f\"‚úÖ Loaded BEST model | iter={ckpt.get('iter')} | best_val={ckpt.get('best_val'):.4f}\")\n",
        "\n",
        "prompt = \"ÿ®ÿ¥ŸÜŸà ÿß€åŸÜ ŸÜ€å ⁄ÜŸàŸÜ ÿ¥⁄©ÿß€åÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ\\n\"\n",
        "temperature = 0.85\n",
        "top_p = 0.9\n",
        "max_new_tokens = 600\n",
        "\n",
        "context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "out = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)[0].tolist()\n",
        "print(decode(out))\n"
      ],
      "metadata": {
        "id": "BDv58NRnqfjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5826be-45e0-4a6f-d1b3-5f73b302277c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded BEST model | iter=26200 | best_val=1.7829\n",
            "ÿ®ÿ¥ŸÜŸà ÿß€åŸÜ ŸÜ€å ⁄ÜŸàŸÜ ÿ¥⁄©ÿß€åÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ\n",
            "⁄©Ÿà ÿ®€å‚ÄåŸÜ€åÿ≥ÿ™ ÿ¥ÿØ ŸÖ€å‚Äåÿ±Ÿà€å ÿ™Ÿà ÿ≤€åŸÜ ÿ±ÿß ÿ®ÿ¥ŸÜŸà\n",
            "⁄ÜŸàŸÜ ÿØÿ± ÿÆŸàÿØ ÿ±Ÿà€å ŸÖ€å‚Äå⁄©ŸÜÿØ ÿÆŸàÿØ ÿßŸà ÿÆÿß⁄© ŸÖ€å‚Äå⁄©ŸÜÿØ\n",
            "ŸÖ€å‚Äå⁄©ŸÜÿØ ÿ™ÿß ÿ®€å‚ÄåŸÜŸÇÿµÿßŸÜ ÿ®€å‚Äå⁄©ŸÜÿØ ÿß€åŸÜ ÿ®ÿ¥ŸÜŸà\n",
            "ÿ¥ÿØ ⁄Øÿ± ŸÖ€å‚Äå⁄©ŸÜÿØ ÿ®ÿ± ÿ±Ÿà€å ⁄©Ÿá ŸÖ€å‚Äå⁄©ŸÜÿØ ÿ≥ŸÑÿ∑ÿßŸÜ ÿ®ÿ¥ŸÜŸà\n",
            "ÿ¥ÿØ ÿÆ€åÿßŸÑ ÿØÿ± ÿÆ€åÿßŸÑ ⁄©Ÿá ÿ¢ŸÜ ÿÆ€åÿßŸÑ ŸÖ€å‚Äå⁄©ŸÜÿØ ÿß€åŸÜ ÿ®ÿßÿ¥ÿØ\n",
            "⁄ÜŸàŸÜ ÿ¢€åÿØ ⁄©Ÿá ÿÆŸàÿ±ÿ¥€åÿØ ÿß€åŸÜ ÿπÿßÿ¥ŸÇÿßŸÜ ÿ®ÿ¥ŸÜŸà\n",
            "ÿßÿ≤ ÿ≥ŸÑÿ∑ÿßŸÜ ÿ®ÿ± ÿ¢€åÿØ ⁄©Ÿá ÿ¢€åÿØ ÿß€åŸÜ Ÿáÿ≥ÿ™ ÿß€åŸÜ ÿØŸÖ\n",
            "ŸÖÿ±ÿß ÿ¢ÿ® ÿ®€å‚Äå⁄ØŸá ⁄ÜŸàŸÜ ÿÆŸàÿØ ÿ±ÿß ⁄ÜŸàŸÜ ÿ®ÿßÿ¥ÿØ ŸÖ€å‚Äå⁄©ŸÜÿØ\n",
            "⁄ØŸá ÿ¢ŸÜ ÿ¨ÿß ÿ®ÿßÿ¥ÿØ ÿßÿ≤ ÿ®€å‚Äå⁄ÜŸàŸÜ ŸÖ€å‚Äå⁄©ŸÜÿØ ÿß€åŸÜ ÿ®ÿßÿ¥ÿØ\n",
            "⁄ÜŸàŸÜ ÿ¢ÿ® ÿ≠€åÿßÿ™ ÿ®€å‚ÄåÿØÿ± ÿ≥ÿ± ÿ≤ÿßŸÜ ÿ®ÿßÿ¥ÿØ ÿß€åŸÜ ÿ®ÿßÿ¥ÿØ\n",
            "⁄ØŸá ⁄©Ÿá ÿ¢ÿ® ⁄Øÿ± ⁄ØŸÑ ÿ±ÿß ŸÜŸÖ€å‚Äå⁄©ŸÜÿØ ÿß€åŸÜ ÿ®ÿßÿ¥ÿØ ÿß€åŸÜ ÿØŸÖ\n",
            "ÿ¢ÿ® ŸÜÿßÿ≤ ⁄ØŸÑÿ¥ŸÜ ÿßÿ≤ ÿß€åŸÜ ÿ®ÿßÿØŸá ÿßÿ≤ ÿ¢ÿ® ⁄ØŸÑÿ¥ŸÜ Ÿà ÿ±Ÿà€å ⁄ÜŸÜ€åŸÜ\n",
            "ÿ®ÿ± ÿß€åŸÜ ÿ®ÿ±ÿ¢Ÿàÿ±ÿØ ÿßÿ≤ ÿ¢ÿ™ÿ¥ ÿ®ÿ±ÿ¢Ÿàÿ±ÿØ ÿßÿ≤ ÿ¢ÿ™ÿ¥ ÿß€åŸÜ ÿÆÿßÿ±\n",
            "⁄ÜŸàŸÜ ÿ¥ÿßÿØ€å ÿ®€å‚ÄåŸæÿß€åÿßŸÜ ÿ¢ŸÜ ⁄Üÿ¥ŸÖ ŸÖ€å‚Äåÿ≤ŸÜÿØ ÿß€åŸÜ ⁄Üÿßÿ±Ÿá ÿ≤ŸÜÿØ ÿß€åŸÜ ŸÖ€å‚Äå⁄©ŸÜÿØ\n",
            "ÿ®ÿ± ÿ¢ÿ™ÿ¥ ÿ¢€åÿØ ŸÜŸàÿ± Ÿà ÿ®ÿßŸÑÿß ÿßŸÜÿØÿ± \n"
          ]
        }
      ]
    }
  ]
}
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5u65-vLqktp",
        "outputId": "da3b6e47-34d9-4acf-cc0c-abd85b9b3da1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# ----------------\n",
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 128   # <-- CHANGED (was 32)\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ----------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# ----------------------------\n",
        "# DATA\n",
        "DATA_PATH = \"/content/drive/MyDrive/rumi_small_clean.txt\"\n",
        "\n",
        "def clean_persian_text(t: str) -> str:\n",
        "    # unify Yeh/Kaf\n",
        "    t = t.replace('Ÿä', '€å').replace('ŸÉ', '⁄©')\n",
        "    # normalize line endings\n",
        "    t = t.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    return t\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = clean_persian_text(f.read())\n",
        "\n",
        "# ----------------------------\n",
        "# Simple BPE (educational)\n",
        "\n",
        "def _merge_seq(seq, pair, new_id):\n",
        "    a, b = pair\n",
        "    out = []\n",
        "    i = 0\n",
        "    L = len(seq)\n",
        "    while i < L:\n",
        "        if i < L - 1 and seq[i] == a and seq[i + 1] == b:\n",
        "            out.append(new_id)\n",
        "            i += 2\n",
        "        else:\n",
        "            out.append(seq[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "class SimpleBPETokenizer:\n",
        "    def __init__(self, id_to_token, merges, unk_token='ÔøΩ'):\n",
        "        self.id_to_token = dict(id_to_token)\n",
        "        self.token_to_id = {t: i for i, t in self.id_to_token.items()}\n",
        "        self.merges = list(merges)  # (a_id, b_id, new_id)\n",
        "        self.unk_token = unk_token\n",
        "        self.unk_id = self.token_to_id.get(unk_token, None)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.id_to_token)\n",
        "\n",
        "    @staticmethod\n",
        "    def _pretokenize(t):\n",
        "        return re.findall(r\"\\s+|\\S+\", t)\n",
        "\n",
        "    @classmethod\n",
        "    def train(cls, text, vocab_size=2048, min_pair_freq=2, return_encoded=True, unk_token='ÔøΩ'):\n",
        "        parts = cls._pretokenize(text)\n",
        "\n",
        "        chars = sorted(set(text))\n",
        "        if unk_token not in chars:\n",
        "            chars = [unk_token] + chars\n",
        "\n",
        "        token_to_id = {ch: i for i, ch in enumerate(chars)}\n",
        "        id_to_token = {i: ch for ch, i in token_to_id.items()}\n",
        "\n",
        "        seqs = [[token_to_id.get(ch, token_to_id[unk_token]) for ch in p] for p in parts]\n",
        "\n",
        "        merges = []\n",
        "        next_id = len(id_to_token)\n",
        "\n",
        "        while next_id < vocab_size:\n",
        "            pair_counts = {}\n",
        "            for seq in seqs:\n",
        "                for a, b in zip(seq, seq[1:]):\n",
        "                    pair_counts[(a, b)] = pair_counts.get((a, b), 0) + 1\n",
        "\n",
        "            if not pair_counts:\n",
        "                break\n",
        "\n",
        "            (best_a, best_b), best_cnt = max(pair_counts.items(), key=lambda kv: (kv[1], kv[0]))\n",
        "            if best_cnt < min_pair_freq:\n",
        "                break\n",
        "\n",
        "            new_id = next_id\n",
        "            next_id += 1\n",
        "\n",
        "            id_to_token[new_id] = id_to_token[best_a] + id_to_token[best_b]\n",
        "            merges.append((best_a, best_b, new_id))\n",
        "\n",
        "            pair = (best_a, best_b)\n",
        "            seqs = [_merge_seq(seq, pair, new_id) for seq in seqs]\n",
        "\n",
        "        tok = cls(id_to_token=id_to_token, merges=merges, unk_token=unk_token)\n",
        "\n",
        "        if return_encoded:\n",
        "            encoded = [tid for seq in seqs for tid in seq]\n",
        "            return tok, encoded\n",
        "        return tok\n",
        "\n",
        "    def encode(self, s):\n",
        "        parts = self._pretokenize(s)\n",
        "        ids = []\n",
        "        for p in parts:\n",
        "            seq = [self.token_to_id.get(ch, self.unk_id) for ch in p]\n",
        "            for a, b, new_id in self.merges:\n",
        "                seq = _merge_seq(seq, (a, b), new_id)\n",
        "            ids.extend(seq)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join(self.id_to_token.get(i, self.unk_token) for i in ids)\n",
        "\n",
        "    def save(self, path):\n",
        "        obj = {\n",
        "            \"id_to_token\": {str(i): t for i, t in self.id_to_token.items()},\n",
        "            \"merges\": self.merges,\n",
        "            \"unk_token\": self.unk_token,\n",
        "        }\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(obj, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            obj = json.load(f)\n",
        "        id_to_token = {int(i): t for i, t in obj[\"id_to_token\"].items()}\n",
        "        merges = [tuple(m) for m in obj[\"merges\"]]\n",
        "        return cls(id_to_token=id_to_token, merges=merges, unk_token=obj.get(\"unk_token\", \"ÔøΩ\"))\n",
        "\n",
        "# ---- tokenizer: save/load\n",
        "BPE_VOCAB_SIZE = 2048\n",
        "BPE_MIN_PAIR_FREQ = 2\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/rumi_bpe_tokenizer.json\"\n",
        "ENCODED_TENSOR_PATH = \"/content/drive/MyDrive/rumi_encoded_2048.pt\"\n",
        "\n",
        "if os.path.exists(TOKENIZER_PATH):\n",
        "    tokenizer = SimpleBPETokenizer.load(TOKENIZER_PATH)\n",
        "    print(\"Loaded tokenizer:\", TOKENIZER_PATH)\n",
        "else:\n",
        "    tokenizer, _ = SimpleBPETokenizer.train(\n",
        "        text,\n",
        "        vocab_size=BPE_VOCAB_SIZE,\n",
        "        min_pair_freq=BPE_MIN_PAIR_FREQ,\n",
        "        return_encoded=True,\n",
        "    )\n",
        "    tokenizer.save(TOKENIZER_PATH)\n",
        "    print(\"Trained + saved tokenizer:\", TOKENIZER_PATH)\n",
        "\n",
        "# cache encoded tokens too (so it won't take minutes every run)\n",
        "if os.path.exists(ENCODED_TENSOR_PATH):\n",
        "    data = torch.load(ENCODED_TENSOR_PATH, map_location=\"cpu\")\n",
        "    print(\"Loaded encoded tokens:\", ENCODED_TENSOR_PATH)\n",
        "else:\n",
        "    encoded_text = tokenizer.encode(text)\n",
        "    data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "    torch.save(data, ENCODED_TENSOR_PATH)\n",
        "    print(\"Encoded + saved tokens:\", ENCODED_TENSOR_PATH)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "encode = tokenizer.encode\n",
        "decode = tokenizer.decode\n",
        "\n",
        "# ----------------------------\n",
        "# Train/val split\n",
        "print(\"device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"tokens:\", len(data), \"vocab_size:\", vocab_size)\n",
        "\n",
        "if len(data) <= block_size:\n",
        "    raise ValueError(f\"len(data)={len(data)} must be > block_size={block_size}\")\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters, device='cpu')\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Sampling: top-p (nucleus)\n",
        "\n",
        "def top_p_filtering(logits, top_p=0.9):\n",
        "    \"\"\"\n",
        "    logits: (B, vocab)\n",
        "    keep the smallest set of tokens whose cumulative prob >= top_p\n",
        "    \"\"\"\n",
        "    sorted_logits, sorted_idx = torch.sort(logits, descending=True, dim=-1)\n",
        "    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "    cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    # mask tokens after top_p mass\n",
        "    mask = cumprobs > top_p\n",
        "    mask[..., 0] = False  # always keep at least 1 token\n",
        "\n",
        "    sorted_logits = sorted_logits.masked_fill(mask, float('-inf'))\n",
        "    # scatter back to original order\n",
        "    filtered = torch.zeros_like(logits).scatter(-1, sorted_idx, sorted_logits)\n",
        "    return filtered\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.shape\n",
        "        k = self.key(x)      # (B,T,head_size)\n",
        "        q = self.query(x)    # (B,T,head_size)\n",
        "\n",
        "        # scale by head_size\n",
        "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)    # (B,T,head_size)\n",
        "        out = wei @ v        # (B,T,head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B,T,n_embd)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T,n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits2 = logits.view(B*T, -1)\n",
        "            targets2 = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits2, targets2)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.8, top_p=0.9):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]  # (B, vocab)\n",
        "\n",
        "            logits = logits / max(temperature, 1e-6)\n",
        "            logits = top_p_filtering(logits, top_p=top_p)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# ----------------------------\n",
        "# Train\n",
        "\n",
        "model = BigramLanguageModel().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# ----------------------------\n",
        "# Generate\n",
        "\n",
        "prompt = \"ÿ®ÿ¥ŸÜŸà ÿß€åŸÜ ŸÜ€å ⁄ÜŸàŸÜ ÿ¥⁄©ÿß€åÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ\\n\"\n",
        "context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "out = model.generate(context, max_new_tokens=600, temperature=0.8, top_p=0.9)[0].tolist()\n",
        "print(decode(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy0FfViBqe2Y",
        "outputId": "ff5d9774-7966-48a2-81d5-ac7787ab1b8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tokenizer: /content/drive/MyDrive/rumi_bpe_tokenizer.json\n",
            "Loaded encoded tokens: /content/drive/MyDrive/rumi_encoded_2048.pt\n",
            "device: cuda\n",
            "GPU: Tesla T4\n",
            "tokens: 424041 vocab_size: 2048\n",
            "0.47168 M parameters\n",
            "step 0: train loss 7.8023, val loss 7.8046\n",
            "step 100: train loss 4.0822, val loss 4.1260\n",
            "step 200: train loss 4.0359, val loss 4.0902\n",
            "step 300: train loss 3.9925, val loss 4.0408\n",
            "step 400: train loss 3.9351, val loss 3.9966\n",
            "step 500: train loss 3.9000, val loss 3.9916\n",
            "step 600: train loss 3.8844, val loss 3.9598\n",
            "step 700: train loss 3.8483, val loss 3.9530\n",
            "step 800: train loss 3.8174, val loss 3.9158\n",
            "step 900: train loss 3.8017, val loss 3.9122\n",
            "step 1000: train loss 3.7761, val loss 3.9047\n",
            "step 1100: train loss 3.7592, val loss 3.8900\n",
            "step 1200: train loss 3.7424, val loss 3.8702\n",
            "step 1300: train loss 3.7223, val loss 3.8655\n",
            "step 1400: train loss 3.6937, val loss 3.8457\n",
            "step 1500: train loss 3.6776, val loss 3.8386\n",
            "step 1600: train loss 3.6630, val loss 3.8355\n",
            "step 1700: train loss 3.6429, val loss 3.8163\n",
            "step 1800: train loss 3.6337, val loss 3.8055\n",
            "step 1900: train loss 3.6178, val loss 3.8084\n",
            "step 2000: train loss 3.6044, val loss 3.7988\n",
            "step 2100: train loss 3.5899, val loss 3.7811\n",
            "step 2200: train loss 3.5769, val loss 3.7898\n",
            "step 2300: train loss 3.5660, val loss 3.7607\n",
            "step 2400: train loss 3.5479, val loss 3.7609\n",
            "step 2500: train loss 3.5331, val loss 3.7454\n",
            "step 2600: train loss 3.5191, val loss 3.7378\n",
            "step 2700: train loss 3.5167, val loss 3.7394\n",
            "step 2800: train loss 3.4957, val loss 3.7217\n",
            "step 2900: train loss 3.4825, val loss 3.7101\n",
            "step 3000: train loss 3.4686, val loss 3.7005\n",
            "step 3100: train loss 3.4491, val loss 3.6841\n",
            "step 3200: train loss 3.4372, val loss 3.6921\n",
            "step 3300: train loss 3.4298, val loss 3.6821\n",
            "step 3400: train loss 3.4186, val loss 3.6711\n",
            "step 3500: train loss 3.4025, val loss 3.6570\n",
            "step 3600: train loss 3.3950, val loss 3.6529\n",
            "step 3700: train loss 3.3768, val loss 3.6493\n",
            "step 3800: train loss 3.3692, val loss 3.6489\n",
            "step 3900: train loss 3.3602, val loss 3.6473\n",
            "step 4000: train loss 3.3438, val loss 3.6446\n",
            "step 4100: train loss 3.3316, val loss 3.6347\n",
            "step 4200: train loss 3.3254, val loss 3.6345\n",
            "step 4300: train loss 3.3143, val loss 3.6361\n",
            "step 4400: train loss 3.3048, val loss 3.6172\n",
            "step 4500: train loss 3.2995, val loss 3.6173\n",
            "step 4600: train loss 3.2925, val loss 3.6190\n",
            "step 4700: train loss 3.2721, val loss 3.6139\n",
            "step 4800: train loss 3.2690, val loss 3.6064\n",
            "step 4900: train loss 3.2591, val loss 3.6020\n",
            "step 4999: train loss 3.2525, val loss 3.5897\n",
            "ÿ®ÿ¥ŸÜŸà ÿß€åŸÜ ŸÜ€å ⁄ÜŸàŸÜ ÿ¥⁄©ÿß€åÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ\n",
            "ŸàÿßŸÜ ÿ±ÿß ÿ®€å‚ÄåÿπŸÇŸÑ ŸÜÿØÿ±€åÿØ €åÿß ÿ∏ÿ±€å\n",
            "ŸÖŸÜ ÿ®Ÿá ÿ±ŸÜ⁄Ø ÿØ€åÿØŸá ŸÖ€å ⁄©ŸÜŸÖ ŸÜŸÖ€å‚ÄåÿØÿßŸÜŸÖ\n",
            "ÿ®Ÿá ⁄ÜŸà ŸÇÿ®ŸÑŸá ÿ™Ÿà ÿ®ÿÆŸàÿ±ÿØŸÖ ⁄©Ÿá ÿ®ÿß ÿ≥Ÿà ÿßÿ≤ ŸÜŸÜ⁄Ø\n",
            "ÿ™⁄©ŸÜ ⁄©Ÿá ŸÖ€å ÿØ€åÿØŸÖ ⁄ÜŸá ÿ®ÿßÿ≤€å Ÿà ÿÆŸàÿßŸáŸÖ\n",
            "ÿ≤ ÿ™Ÿà ÿ≤ ÿ¥ÿ® ÿ™Ÿà ÿ±ÿß ÿ®ÿ± ÿ¢ŸÜ ÿ±ÿ¨ÿßÿß€å ÿ±ÿß\n",
            "ÿßÿ≤ ÿ¢ŸÜ⁄© ÿ™Ÿà ⁄©Ÿá ÿ∫ÿ±ÿßÿ™ ŸÖÿ±ÿß ÿ®ÿ¨ŸÜÿ®ÿßŸÜ\n",
            "Ÿáÿ± ⁄©ÿ≥ ÿ®Ÿá ÿ™Ÿà ÿ±ÿß ÿ™Ÿà ÿ¢ ÿ≤ ŸÖÿ´ŸÑ ⁄©ŸàŸÅÿ™Ÿá\n",
            "⁄ÜŸàŸÜ ÿßÿ≤ Ÿáÿ± ÿ®Ÿà€å Ÿà ÿßÿ≥ÿ™ ÿßÿ≤ ŸÇÿ± ÿ™Ÿà ÿ±ÿß ÿ®ÿßÿ∑ŸÜ\n",
            "ÿØÿ± ÿ¥ÿ® ÿ≤ ÿ™Ÿà ÿ®ÿßŸÇ€å ŸÜÿ®ŸàÿØ ÿßÿ≤ ÿ™Ÿà ÿ®Ÿá ÿ™Ÿà ÿÆŸàÿ¥\n",
            "ÿß€å ÿ¥ŸÖÿπ ÿ≤ ÿ∫ŸÖ Ÿà ŸáŸÖ ÿ≠ÿ±€åŸÅÿßŸÜ ŸÖ⁄©ŸÜ\n",
            "⁄ØŸà€å€å ÿ®€å‚ÄåÿßŸà ⁄©ŸÜÿØ Ÿà ÿ¢ŸÜ⁄ØŸá Ÿà ÿ™Ÿà ÿ±ÿß ÿ¨Ÿà\n",
            "ŸÖÿ¨ŸÜÿ®ÿßŸÜ ÿ±ÿß ÿ™Ÿà ÿ±ÿß ŸÜ€å ÿ±ÿß ÿ™Ÿà ÿ±ÿß ÿ®ŸàÿØ ŸÜ€åÿ≥ÿ™\n",
            "⁄ØŸÅÿ™ŸÖ ÿ®ÿ± ÿ≥ÿ± ŸÖ€å‚Äåÿ®ÿßÿ¥ ÿØÿ± €å⁄© ÿØŸà ÿ±ÿ≥ÿØ ÿ®ÿÆŸàÿ±\n",
            "ŸÜŸá ⁄©Ÿá ÿ≤ ÿØÿ≥ÿ™ ÿ™Ÿà ÿßÿ≤ ÿß€åŸÜ ÿ≥Ÿà€å ÿ™Ÿà ŸÜÿ®ÿØ€å\n",
            "ÿß€å ÿ®ÿ≥ÿ™Ÿá ⁄©Ÿá ÿ≤ŸÜÿØŸá ÿß€å ŸÅÿ™ŸÜŸá ÿ™Ÿà ÿ±ÿß ÿ®ÿ±ÿÆÿßÿ≥ÿ™\n",
            "ÿß€å ÿÆŸàÿßÿ® Ÿà ⁄ØŸÑ ÿ™Ÿà €å⁄©€å ŸáŸÖŸá ÿ≤ ÿµŸàÿ±ÿ™ ÿ®Ÿá ÿ®€å‚Äå⁄ØŸÅÿ™\n",
            "⁄Øÿ± ÿßÿ≤ ÿß€åŸÜ ÿ≤€åÿ± ÿ™Ÿà ŸÖ€å‚ÄåÿØŸà€å Ÿà ÿ¥ÿ±ÿ® ÿ™Ÿà\n",
            "ÿßŸÅÿ≥ÿ± ŸÖÿ±ÿß ÿßÿ≤ ÿ¢ŸÜ ÿ¨ŸáÿßŸÜ ÿßÿ≥ÿ™ ÿ®ÿ± ÿ™ŸÜ ÿ¢ŸÜ\n",
            "ÿ®Ÿá ÿ¢ŸÜ⁄© ÿßÿ≤ ÿ™Ÿà ÿ±ÿß ÿ®€å‚Äåÿ≤€åÿ± ÿßŸà Ÿà ŸÖÿßŸá€å ÿ™Ÿà\n",
            "ÿß€å ÿÆŸÜÿØŸá Ÿà ÿßŸÅÿ≥ÿßŸÜŸá Ÿà ÿ¨ÿßÿ®ŸÇ ÿ®ÿØ€å ÿ™Ÿà\n",
            "⁄ÜŸà ÿ™Ÿà ÿ®€å‚Äå⁄Üÿ¥€å ÿ™ŸÜ ÿßÿ≤ ÿ™Ÿà ŸÖÿ±Ÿà ÿ¨ÿßŸÜ ÿ™Ÿà\n",
            "⁄ØŸÅÿ™ŸÖ ⁄©Ÿá ÿØÿ≥ÿ™ ÿßŸà ÿ¢ŸÜ ÿ®Ÿá ÿ¨ÿßŸÜ ŸÖŸá ÿßŸà ÿ™Ÿà\n",
            "Ÿáÿ± ÿØŸà ⁄ØŸà€å€å ⁄©Ÿá Ÿæÿß€å ÿ™Ÿà ÿ≤ ÿ®ÿß ÿ™Ÿà ÿ±ÿß ÿ™Ÿà\n",
            "ÿßÿ≤ ÿßŸà ÿ™Ÿà ŸÖÿ±ÿß ŸÖÿßŸá ÿ±Ÿá ÿÆŸàÿ¥ ÿÆŸàÿ¥ ÿ™Ÿà ÿ±ÿß\n",
            "ÿØÿ± ÿ¨ŸÖÿßŸÑ ÿ™Ÿà ⁄©ÿ≥€å ÿ±ÿß ⁄©Ÿá ŸÖŸÜ ÿ®Ÿá ŸÖŸÜ ŸÖ€å‚ÄåÿØÿßŸÜŸÖ\n",
            "ÿ≤ÿßŸÜ ÿ≤ ÿ™Ÿà ÿ±ÿß Ÿà ÿ∫ŸÖ ÿ™Ÿà ŸÜÿ®ÿß Ÿà ⁄ØŸÑ ÿ™Ÿà\n",
            "ÿ¥ÿ® ⁄ÜŸà ŸáŸÖ ÿ™Ÿà ÿ®Ÿá ÿ™Ÿà ⁄ÜŸá ÿ®ÿßÿ¥ÿØ ⁄©Ÿá ÿØÿ± ÿ™Ÿà ŸÖŸÜ\n",
            "ÿ®ÿ±ÿß€å ÿ™Ÿà ÿµÿØ €å⁄©€å Ÿà ŸÖÿßŸá ÿ®Ÿá ÿØÿ≥ÿ™ŸÖ Ÿà ⁄Üÿ±ÿßÿ∫\n",
            "ÿ®Ÿá ÿØŸÑ ÿ®Ÿá ⁄ÜŸá ÿØÿßŸÜŸÖ ÿ≤ ÿ≥ÿ± ÿ≤ ÿ±Ÿá ÿ™Ÿà ÿ™Ÿà\n",
            "ÿ®Ÿá ÿ®Ÿá ÿ≠ŸÑŸÇŸá ŸÅŸÑ⁄© ÿ¢ŸÜ ⁄ÜŸá ÿ¨ÿß ÿ™Ÿà \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDv58NRnqfjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
